docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d,10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
q()
q()
"Hello World!"
5 + 5
print("Hello World!")
# This is a comment by midhun
plot(1:10)
for (x in 1:10)
{
  print(x)
}
age <- 40
text <- "KRCE"
paste("R is", text) # Concatenate Elements
var1 <- var2 <- var3 <- "KRGi
# numeric
x <- 10.5
class(x)
max(5, 10, 15)
min(5, 10, 15)
sqrt(16)
abs(-4.7)
ceiling(1.4)
floor(1.4)
str <- "Hello World!"
nchar(str)
grepl("Hello", str)
grepl("X", str)
a<- 200
b <- 33
x <- 5
y <- 16
x + y
x - y
x * y
y / x
y %/% x
y %% x
y ^ x
x <- c(2, 8, 3)
y <- c(6, 4, 1)
x + y
x > y
if (b > a) {
  print("b is greater than a")
} else if (a == b) {
  print("a and b are equal")
} else {
  print("a is greater than b")
}
[1] "a is greater than b"
#loop
i <- 1
while (i < 6) {
  print(i)
  i <- i + 1
}
for (i in 1:10)
{
  print(i)
}
dice <- c(1, 2, 3, 4, 5, 6)
for (x in dice) {
  print(x)
}
[1] 1
#function
my_function <- function() {
  print("Hello World!  **  KRCE   **  ")
}
my_function() # call the function named my_function
fruits <- c("banana", "apple", "orange", "mango", "lemon")
numbers <- c(13, 3, 5, 7, 20, 2)
sort(fruits)  # Sort a string
sort(numbers)
thismatrix <- matrix(c(1,2,3,4,5,6), nrow = 3, ncol = 2)
plot(1:10, type="l")
x <- c(5,7,8,7,2,2,9,4,11,12,9,6)
y <- c(99,86,87,88,111,103,87,94,78,77,85,86)
plot(x, y ,col="blue", cex=2)
x <- c(10,20,30,40)
# Display the pie chart
pie(x)
filePath <- "https://github.com/Midhun-c/Login/blob/main/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d,10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
filePath <- "https://midhun-c.github.io/Login/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d,10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
q()
clear
filePath <- "https://midhun-c.github.io/Login/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d,10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
pageRank <- function(M, num_iter = 1, d = 0.85) {
  N <- ncol(M)
  v <- rep(1/N, N)
  M_hat <- d * M + (1 - d) / N
  for (i in 1:num_iter) {
    v <- M_hat %*% v
  }
  return(v)
}
M <- matrix(c(0, 0, 0, 0, 1,
              0.5, 0, 0, 0, 0,
              0.5, 1, 0, 0, 0,
              0, 0, 1, 0.5, 0,
              0, 0, 0, 0.5, 0), nrow = 5, byrow = TRUE)
v <- pageRank(M, 100, 0.85)
print(v)
pageRank <- function(M, num_iter = 1, d = 0.85) {
  N <- ncol(M)
  v <- rep(1/N, N)
  M_hat <- d * M + (1 - d) / N
  for (i in 1:num_iter) {
    v <- M_hat %*% v
  }
  return(v)
}
M <- matrix(c(0, 0, 0, 0, 1,
              0.5, 0, 0, 0, 0,
              0.5, 1, 0, 0, 0,
              0, 0, 1, 0.5, 0,
              0, 0, 0, 0.5, 0), nrow = 5, byrow = TRUE)
v <- pageRank(M, 100, 0.85)
print(v)
q()
q()
q()
filePath <- "https://midhun-c.github.io/Login/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
findFreqTerms(dtm, lowfreq = 4)
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
head(d,10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
filePath <- "https://midhun-c.github.io/Login/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
filePath <- "https://midhun-c.github.io/Login/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
install.package("wordcloud")
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
pageRank <- function(M, num_iter = 1, d = 0.85) {
  N <- ncol(M)
  v <- rep(1/N, N)
  M_hat <- d * M + (1 - d) / N
  for (i in 1:num_iter) {
    v <- M_hat %*% v
  }
  return(v)
}
M <- matrix(c(0, 0, 0, 0, 1,
              0.5, 0, 0, 0, 0,
              0.5, 1, 0, 0, 0,
              0, 0, 1, 0.5, 0,
              0, 0, 0, 0.5, 0), nrow = 5, byrow = TRUE)
v <- pageRank(M, 100, 0.85)
print(v)
# Install
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator 
install.packages("RColorBrewer") # color palettes
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
filePath <- "https://midhun-c.github.io/Login/KRCE.txt"
text<- readLines(filePath)
docs <- Corpus(VectorSource(text))
inspect(docs)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
 docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#findFreqTerms(dtm, lowfreq = 4)
#findAssocs(dtm, terms = "freedom", corlimit = 0.3)
#head(d,10)
#barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
# Step 1: Load Data data(iris) # Load Iris dataset
df <- iris # Create a dataframe
# Step 2: Preprocess Data (if needed)
# Skip this step if preprocessing is not required
# Step 3: Choose the Number of Clusters (K) k <- 3 # Number of clusters
# Step 4: Perform Clustering set.seed(123) # For reproducibility
kmeans_result <- kmeans(df[, -5], centers = k) # Perform clustering
# Step 5: Visualize Results library(ggplot2)
ggplot(df, aes(Petal.Length, Petal.Width, color = factor(kmeans_result$cluster))) + geom_point() +
geom_point(data=as.data.frame(kmeans_result$centers[,c("Petal.Length", "Petal.Width")]), color = "black", size = 3, shape = 17) +
labs(title = "K-means Clustering of Iris Dataset") + theme_minimal()
# Step 6: Interpret Results
# Analyze cluster centroids, cluster sizes, etc.
# Step 1: Load Data data(iris) # Load Iris dataset
df <- iris # Create a dataframe
# Step 2: Preprocess Data (if needed)
# Skip this step if preprocessing is not required
# Step 3: Choose the Number of Clusters (K) 
k <- 3 # Number of clusters
# Step 4: Perform Clustering set.seed(123) # For reproducibility
kmeans_result <- kmeans(df[, -5], centers = k) # Perform clustering
# Step 5: Visualize Results library(ggplot2)
ggplot(df, aes(Petal.Length, Petal.Width, color = factor(kmeans_result$cluster))) + geom_point() +
geom_point(data=as.data.frame(kmeans_result$centers[,c("Petal.Length", "Petal.Width")]), color = "black", size = 3, shape = 17) +
labs(title = "K-means Clustering of Iris Dataset") + theme_minimal()
# Step 6: Interpret Results
# Analyze cluster centroids, cluster sizes, etc.
# Step 1: Load Data data(iris) # Load Iris dataset
library("ggplot2")
df <- iris # Create a dataframe
# Step 2: Preprocess Data (if needed)
# Skip this step if preprocessing is not required
# Step 3: Choose the Number of Clusters (K) 
k <- 3 # Number of clusters
# Step 4: Perform Clustering set.seed(123) # For reproducibility
kmeans_result <- kmeans(df[, -5], centers = k) # Perform clustering
# Step 5: Visualize Results library(ggplot2)
ggplot(df, aes(Petal.Length, Petal.Width, color = factor(kmeans_result$cluster))) + geom_point() +
geom_point(data=as.data.frame(kmeans_result$centers[,c("Petal.Length", "Petal.Width")]), color = "black", size = 3, shape = 17) +
labs(title = "K-means Clustering of Iris Dataset") + theme_minimal()
# Step 6: Interpret Results
# Analyze cluster centroids, cluster sizes, etc.
# Step 1: Load Data data(iris) # Load Iris dataset
df <- iris # Create a dataframe
# Step 2: Preprocess Data (if needed)
# Skip this step if preprocessing is not required
# Step 3: Choose the Number of Clusters (K) k <- 3 # Number of clusters
# Step 4: Perform Clustering library(cluster)
set.seed(123) # For reproducibility
pam_result <- pam(df[, -5], k) # Perform clustering
# Step 5: Visualize Results library(ggplot2)
ggplot(df, aes(Petal.Length, Petal.Width, color = factor(pam_result$clustering))) + geom_point() +
geom_point(data=as.data.frame(pam_result$medoids[,c("Petal.Length", "Petal.Width")]), color = "black", size = 3, shape = 17) +
labs(title = "K-medoids (PAM) Clustering of Iris Dataset") + theme_minimal()
# Step 6: Interpret Results
# Analyze cluster medoids, cluster sizes, etc.
# Step 1: Load Data data(iris) # Load Iris dataset
df <- iris # Create a dataframe
# Step 2: Preprocess Data (if needed)
# Skip this step if preprocessing is not required
# Step 3: Choose the Number of Clusters (K) 
k <- 3 # Number of clusters
# Step 4: Perform Clustering 
library("cluster")
set.seed(123) # For reproducibility
pam_result <- pam(df[, -5], k) # Perform clustering
# Step 5: Visualize Results 
library("ggplot2")
ggplot(df, aes(Petal.Length, Petal.Width, color = factor(pam_result$clustering))) + geom_point() +
geom_point(data=as.data.frame(pam_result$medoids[,c("Petal.Length", "Petal.Width")]), color = "black", size = 3, shape = 17) +
labs(title = "K-medoids (PAM) Clustering of Iris Dataset") + theme_minimal()
# Step 6: Interpret Results
# Analyze cluster medoids, cluster sizes, etc.
q()
q()
